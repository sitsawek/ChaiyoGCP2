{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size = \"6\">Perform Foundational Data, ML, and AI Tasks in Google Cloud<center>**\n",
    "***\n",
    "<center><font size = \"2\">Prepared by: Sitsawek Sukorn<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI: Qwik Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Google Cloud services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Cloud Shell, use gcloud to enable the services used in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud services enable \\\n",
    "  compute.googleapis.com \\\n",
    "  iam.googleapis.com \\\n",
    "  iamcredentials.googleapis.com \\\n",
    "  monitoring.googleapis.com \\\n",
    "  logging.googleapis.com \\\n",
    "  notebooks.googleapis.com \\\n",
    "  aiplatform.googleapis.com \\\n",
    "  bigquery.googleapis.com \\\n",
    "  artifactregistry.googleapis.com \\\n",
    "  cloudbuild.googleapis.com \\\n",
    "  container.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex AI custom service account for Vertex Tensorboard integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create custom service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_ID=vertex-custom-training-sa\n",
    "gcloud iam service-accounts create $SERVICE_ACCOUNT_ID  \\\n",
    "    --description=\"A custom service account for Vertex custom training with Tensorboard\" \\\n",
    "    --display-name=\"Vertex AI Custom Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant it access to GCS for writing and retrieving Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ID=$(gcloud config get-value core/project)\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \\\n",
    "    --role=\"roles/storage.admin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant it access to your BigQuery data source to read data into your TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \\\n",
    "    --role=\"roles/bigquery.admin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant it access to Vertex AI for running model training, deployment, and explanation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \\\n",
    "    --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Vertex AI Workbench notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and launch a Vertex AI Workbench notebook:\n",
    "\n",
    "- In the Navigation Menu Navigation menu icon, click Vertex AI > Workbench.\n",
    "\n",
    "- On the Workbench page, click New Notebook.\n",
    "\n",
    "- In the Customize instance menu, select TensorFlow Enterprise and choose the latest version of TensorFlow Enterprise 2.x (with LTS) > Without GPUs.\n",
    "\n",
    "- Name the notebook.\n",
    "\n",
    "- Set Region to us-central1 and Zone to any zone within the designated region.\n",
    "\n",
    "- In the Notebook properties, click the pencil icon pencil icon to edit the instance properties.\n",
    "\n",
    "- Scroll down to Machine configuration and select e2-standard-2 for Machine type.\n",
    "\n",
    "- Leave the remaining fields at their default and click Create.\n",
    "\n",
    "After a few minutes, the Workbench page lists your instance, followed by Open JupyterLab.\n",
    "\n",
    "- Click Open JupyterLab to open JupyterLab in a new tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the example repo within your Workbench instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clone the training-data-analyst repository in your JupyterLab instance:\n",
    "\n",
    "- In JupyterLab, click the Terminal icon to open a new terminal.\n",
    "\n",
    "- At the command-line prompt, type the following command and press ENTER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To confirm that you have cloned the repository, in the left panel, double click the training-data-analyst folder to see its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take several minutes for the notebook to clone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install lab dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the following to go to the training-data-analyst/self-paced-labs/vertex-ai/vertex-ai-qwikstart folder, then pip install requirements.txt to install lab dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd training-data-analyst/self-paced-labs/vertex-ai/vertex-ai-qwikstart\n",
    "pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigate to lab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In your notebook, navigate to training-data-analyst > self-paced-labs > vertex-ai > vertex-ai-qwikstart, and open lab_exercise.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continue the lab in the notebook, and run each cell by clicking the Run icon at the top of the screen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can execute the code in a cell with SHIFT + ENTER.\n",
    "\n",
    "Read the narrative and make sure you understand what's happening in each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size = \"6\">Dataprep: Qwik Start<center>**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket in your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Cloud Console, select Navigation menu(Navigation menu icon) > Cloud Storage > Buckets.\n",
    "\n",
    "- Click Create bucket.\n",
    "\n",
    "- In the Create a bucket dialog, Name the bucket a unique name. Leave other settings at their default value.\n",
    "\n",
    "Note: Learn more about naming buckets from Bucket naming guidelines.\n",
    "\n",
    "- Click Create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Cloud Dataprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select Navigation menu > Dataprep.\n",
    "\n",
    "- Check to accept the Google Dataprep Terms of Service, then click Accept.\n",
    "\n",
    "- Check to authorize sharing your account information with Trifacta, then click Agree and Continue.\n",
    "\n",
    "- Click Allow to allow Trifacta to access project data.\n",
    "\n",
    "- Click your student username to sign in to Cloud Dataprep by Trifacta. Your username is the Username in the left panel in your lab.\n",
    "\n",
    "- Click Allow to grant Cloud Dataprep access to your Google Cloud lab account.\n",
    "\n",
    "- Check to agree to Trifacta Terms of Service, and then click Accept.\n",
    "\n",
    "- Click Continue on the First time setup screen to create the default storage location.\n",
    "\n",
    "Dataprep opens.\n",
    "\n",
    "- Click on the Dataprep icon on the top left corner to go to the home screen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud Dataprep uses a flow workspace to access and manipulate datasets.\n",
    "\n",
    "- Click Flows icon, then the Create button, then select Blank Flow :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click on Untitled Flow, then name and describe the flow. Since this lab uses 2016 data from the United States Federal Elections Commission 2016, name the flow \"FEC-2016\", and then describe the flow as \"United States Federal Elections Commission 2016\".\n",
    "\n",
    "- Click OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you import and add data to the FEC-2016 flow.\n",
    "\n",
    "- Click Add Datasets, then select the Import Datasets link.\n",
    "\n",
    "- In the left menu pane, select Cloud Storage to import datasets from Cloud Storage, then click on the pencil to edit the file path.\n",
    "\n",
    "- Type gs://spls/gsp105 in the Choose a file or folder text box, then click Go.\n",
    "You may have to widen the browser window to see the Go and Cancel buttons.\n",
    "\n",
    "- Click us-fec/.\n",
    "\n",
    "- Click the + icon next to cn-2016.txt to create a dataset shown in the right pane. Click on the title in the dataset in the right pane and rename it \"Candidate Master 2016\".\n",
    "\n",
    "- In the same way add the itcont-2016-orig.txt dataset, and rename it \"Campaign Contributions 2016\".\n",
    "\n",
    "- Both datasets are listed in the right pane; click Import & Add to Flow.\n",
    "\n",
    "You see both datasets listed as a flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the candidate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, the Candidate Master 2016 dataset is selected. In the right pane, click Edit Recipe.\n",
    "Candidate Master 2016 dataset page\n",
    "\n",
    "The Candidate Master 2016 Transformer page opens in the grid view.\n",
    "\n",
    "The Transformer page is where you build your transformation recipe and see the results applied to the sample. When you are satisfied with what you see, execute the job against your dataset.\n",
    "\n",
    "- Each of the column heads have a Name and value that specify the data type. To see data types, click the column icon:\n",
    "\n",
    "- Notice also that when you click the name of the column, a Details panel opens on the right.\n",
    "\n",
    "- Click X in the top right of the Details panel to close the Details panel.\n",
    "\n",
    "In the following steps you explore data in the grid view and apply transformation steps to your recipe.\n",
    "\n",
    "- Column5 provides data from 1990-2064. Widen column5 (like you would on a spreadsheet) to separate each year. Click to select the tallest bin, which represents the year 2016.\n",
    "\n",
    "This creates a step where these values are selected.\n",
    "\n",
    "- In the Suggestions panel on the right, in the Keep rows section, click Add to add this step to your recipe.\n",
    "\n",
    "The Recipe panel on the right now has the following step:\n",
    "\n",
    "Keep rows where(DATE(2016, 1, 1) <= column5) && (column5 < DATE(2018, 1, 1))\n",
    "\n",
    "- In Column6 (State), hover over and click on the mismatched (red) portion of the header to select the mismatched rows.\n",
    "\n",
    "Scroll down to the bottom (highlighted in red) find the mismatched values and notice how most of these records have the value \"P\" in column7, and \"US\" in column6. The mismatch occurs because column6 is marked as a \"State\" column (indicated by the flag icon), but there are non-state (such as \"US\") values.\n",
    "\n",
    "- To correct the mismatch, click X in the top of the Suggestions panel to cancel the transformation, then click on the flag icon in Column6 and change it to a \"String\" column.\n",
    "\n",
    "There is no longer a mismatch and the column marker is now green.\n",
    "\n",
    "- Filter on just the presidential candidates, which are those records that have the value \"P\" in column7. In the histogram for column7, hover over the two bins to see which is \"H\" and which is \"P\". Click the \"P\" bin.\n",
    "\n",
    "- In the right Suggestions panel, click Add to accept the step to the recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the Contributions file and join it to the Candidates file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Join page, you can add your current dataset to another dataset or recipe based on information that is common to both datasets.\n",
    "\n",
    "Before you join the Contributions file to the Candidates file, clean up the Contributions file.\n",
    "\n",
    "- Click on FEC-2016 (the dataset selector) at the top of the grid view page.\n",
    "\n",
    "- Click to select the grayed out Campaign Contributions 2016.\n",
    "\n",
    "- In the right pane, click Add > Recipe, then click Edit Recipe.\n",
    "\n",
    "- Click the recipe icon at the top right of the page, then click Add New Step.\n",
    "\n",
    "Remove extra delimiters in the dataset.\n",
    "\n",
    "- Insert the following Wrangle language command in the Search box:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacepatterns col: * with: '' on: `{start}\"|\"{end}` global: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformation Builder parses the Wrangle command and populates the Find and Replace transformation fields.\n",
    "\n",
    "- Click Add to add the transform to the recipe.\n",
    "\n",
    "- Add another new step to the recipe. Click New Step, then type \"Join\" in the Search box.\n",
    "\n",
    "- Click Join datasets to open the Joins page.\n",
    "\n",
    "- Click on \"Candidate Master 2016\" to join with Campaign Contributions 2016, then Accept in the bottom right.\n",
    "\n",
    "- On the right side, hover in the Join keys section, then click on the pencil (Edit icon).\n",
    "\n",
    "Dataprep infers common keys. There are many common values that Dataprep suggests as Join Keys.\n",
    "\n",
    "- In the Add Key panel, in the Suggested join keys section, click column2 = column11.\n",
    "\n",
    "- Click Save and Continue.\n",
    "\n",
    "Columns 2 and 11 open for your review.\n",
    "\n",
    "- Click Next, then check the checkbox to the left of the \"Column\" label to add all columns of both datasets to the joined dataset.\n",
    "\n",
    "- Click Review, and then Add to Recipe to return to the grid view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a useful summary by aggregating, averaging, and counting the contributions in Column 16 and grouping the candidates by IDs, names, and party affiliation in Columns 2, 24, 8 respectively.\n",
    "\n",
    "- At the top of the Recipe panel on the right, click on New Step and enter the following formula in the Transformation search box to preview the aggregated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pivot value:sum(column16),average(column16),countif(column16 > 0) group: column2,column24,column8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial sample of the joined and aggregated data is displayed, representing a summary table of US presidential candidates and their 2016 campaign contribution metrics.\n",
    "\n",
    "- Click Add to open a summary table of major US presidential candidates and their 2016 campaign contribution metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make the data easier to interpret by renaming the columns.\n",
    "\n",
    "- Add each of the renaming and rounding steps individually to the recipe by clicking New Step, then enter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename type: manual mapping: [column24,'Candidate_Name'], [column2,'Candidate_ID'],[column8,'Party_Affiliation'], [sum_column16,'Total_Contribution_Sum'], [average_column16,'Average_Contribution_Sum'], [countif,'Number_of_Contributions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then click Add.\n",
    "\n",
    "- Add in this last New Step to round the Average Contribution amount:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set col: Average_Contribution_Sum value: round(Average_Contribution_Sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then click Add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size = \"6\">Dataflow: Qwik Start - Templates<center>**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the Dataflow API is successfully enabled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure access to the necessary API, restart the connection to the Dataflow API.\n",
    "\n",
    "- In the Cloud Console, enter \"Dataflow API\" in the top search bar. Click on the result for Dataflow API.\n",
    "\n",
    "- Click Manage.\n",
    "\n",
    "- Click Disable API.\n",
    "\n",
    "If asked to confirm, click Disable.\n",
    "\n",
    "- Click Enable.\n",
    "\n",
    "When the API has been enabled again, the page will show the option to disable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud BigQuery dataset and table Using Cloud Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a BigQuery dataset and table.\n",
    "\n",
    "Note: This section uses the bq command-line tool. Skip down if you want to run through this lab using the console.\n",
    "\n",
    "- Run the following command to create a dataset called taxirides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bq mk taxirides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset '' successfully created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the following command to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bq mk \\\n",
    "--time_partitioning_field timestamp \\\n",
    "--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\\\n",
    "timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\\\n",
    "passenger_count:integer -t taxirides.realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 'myprojectid:taxirides.realtime' successfully created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our table instantiated, let's create a bucket.\n",
    "\n",
    "- Run the following commands to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export BUCKET_NAME=\"<your-unique-name>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gsutil mb gs://$BUCKET_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud BigQuery dataset and table using the Cloud Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't go through this section if you've done the command-line setup!\n",
    "\n",
    "- From the left-hand menu, in the Big Data section, click on BigQuery.\n",
    "\n",
    "- Then click Done.\n",
    "\n",
    "- Click on the three dots next to your project name under the Explorer section, then click Create dataset.\n",
    "\n",
    "- Input taxirides as your dataset ID:\n",
    "\n",
    "- Select us (multiple regions in United States) in Data location.\n",
    "\n",
    "- Leave all of the other default settings in place and click CREATE DATASET.\n",
    "\n",
    "- You should now see the taxirides dataset underneath your project ID in the left-hand console.\n",
    "\n",
    "- Click on the three dots next to taxirides dataset and select Open.\n",
    "\n",
    "- Then select CREATE TABLE in the right-hand side of the console.\n",
    "\n",
    "- In the Destination > Table Name input, enter realtime.\n",
    "\n",
    "- Under Schema, toggle the Edit as text slider and enter the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,\n",
    "meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, click Create table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go back to the Cloud Console and navigate to Cloud Storage > Browser > Create bucket.\n",
    "\n",
    "- Give your bucket a unique name.\n",
    "\n",
    "- Leave all other default settings, then click Create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the Navigation menu, find the Analytics section and click on Dataflow.\n",
    "\n",
    "- Click on + Create job from template at the top of the screen.\n",
    "\n",
    "- Enter iotflow as the Job name for your Cloud Dataflow job and select us-east1 for Regional Endpoint.\n",
    "\n",
    "- Under Dataflow Template, select the Pub/Sub Topic to BigQuery template.\n",
    "\n",
    "- Under Input Pub/Sub topic, click Enter Topic Manually and enter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "projects/pubsub-public-data/topics/taxirides-realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Under BigQuery output table, enter the name of the table that was created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<myprojectid>:taxirides.realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add your bucket as Temporary Location:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs://Your_Bucket_Name/temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit queries using standard SQL.\n",
    "\n",
    "- In the BigQuery Editor field add the following, replacing myprojectid with the Project ID from the Qwiklabs page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM `myprojectid.taxirides.realtime` LIMIT 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now click RUN.\n",
    "\n",
    "If you run into any issues or errors, run the query again (the pipeline takes a minute to start up.)\n",
    "\n",
    "- When the query runs successfully, you'll see the output in the Query Results panel as shown below:\n",
    "\n",
    "Great work! You just pulled 1000 taxi rides from a Pub/Sub topic and pushed them to a BigQuery table. As you saw firsthand, templates are a practical, easy-to-use way to run Dataflow jobs. Be sure to check out, in the Dataflow Documentation, some other Google Templates in the Get started with Google-provided templates Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size = \"6\">Dataflow: Qwik Start - Python<center>**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Cloud Console, click on Navigation menu and then click on Cloud Storage.\n",
    "\n",
    "- Click Create bucket.\n",
    "\n",
    "- In the Create bucket dialog, specify the following attributes:\n",
    "\n",
    "Name: A unique bucket name. Do not include sensitive information in the bucket name, as the bucket namespace is global and publicly visible.\n",
    "\n",
    "- Location type: Multi-region\n",
    "\n",
    "- Location: us\n",
    "\n",
    "A location where bucket data will be stored.\n",
    "\n",
    "- Click Create.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pip and the Cloud Dataflow SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest Cloud Dataflow SDK for Python requires a Python version >= 3.7.\n",
    "\n",
    "- To ensure you are running the process with the correct version, run the Python3.9 Docker Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID python:3.9 /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command pulls a Docker container with the latest stable version of Python 3.9 and then opens up a command shell for you to run the following commands inside your container.\n",
    "\n",
    "- After the container is running, install the latest version of the Apache Beam for Python by running the following command from a virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install 'apache-beam[gcp]'==2.42.0rc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see some warnings returned that are related to dependencies. It is safe to ignore them for this lab.\n",
    "\n",
    "- Run the wordcount.py example locally by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m apache_beam.examples.wordcount --output OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You installed google-cloud-dataflow but are executing wordcount with Apache_beam. The reason for this is that Cloud Dataflow is a distribution of Apache Beam.\n",
    "\n",
    "You may see a message similar to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
    "INFO:oauth2client.client:Attempting refresh to obtain initial access_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This message can be ignored.\n",
    "\n",
    "- You can now list the files that are on your local cloud environment to get the name of the OUTPUT_FILE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copy the name of the OUTPUT_FILE and cat into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cat <file name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an example pipeline remotely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set the BUCKET environment variable to the bucket you created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "BUCKET=gs://<bucket name provided earlier>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now you'll run the wordcount.py example remotely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m apache_beam.examples.wordcount --project $DEVSHELL_PROJECT_ID \\\n",
    "  --runner DataflowRunner \\\n",
    "  --staging_location $BUCKET/staging \\\n",
    "  --temp_location $BUCKET/temp \\\n",
    "  --output $BUCKET/results/output \\\n",
    "  --region us-west1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your output, wait until you see the message:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOB_MESSAGE_DETAILED: Workers have started successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that your job succeeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open the Navigation menu and click Dataflow from the list of services.\n",
    "\n",
    "You should see your wordcount job with a status of Running at first.\n",
    "\n",
    "- Click on the name to watch the process. When all the boxes are checked off, you can continue watching the logs in Cloud Shell.\n",
    "\n",
    "The process is complete when the status is Succeeded.\n",
    "\n",
    "- Click Navigation menu > Cloud Storage in the Cloud Console.\n",
    "\n",
    "- Click on the name of your bucket. In your bucket, you should see the results and staging directories.\n",
    "\n",
    "- Click on the results folder and you should see the output files that your job created:\n",
    "\n",
    "- Click on a file to see the word counts it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size = \"6\">Dataproc: Qwik Start - Console<center>**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Cloud Dataproc API is enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Dataproc cluster in Google Cloud, the Cloud Dataproc API must be enabled. To confirm the API is enabled:\n",
    "\n",
    "- Click Navigation menu > APIs & Services > Library:\n",
    "\n",
    "- Type Cloud Dataproc in the Search for APIs & Services dialog. The console will display the Cloud Dataproc API in the search results.\n",
    "\n",
    "- Click on Cloud Dataproc API to display the status of the API. If the API is not already enabled, click the Enable button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Cloud Platform Console, select Navigation menu > Dataproc > Clusters, then click Create cluster.\n",
    "\n",
    "- Click Create for Cluster on Compute Engine.\n",
    "\n",
    "- Set the following fields for your cluster and accept the default values for all other fields:\n",
    "\n",
    "Note: In the Configure nodes section ensure both the Master node and Worker nodes are set to the correct Machine Series and Machine Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: A Zone is a special multi-region namespace that is capable of deploying instances into all Google Compute zones globally. You can also specify distinct regions, such as us-central1 or europe-west1, to isolate resources (including VM instances and Cloud Storage) and metadata storage locations utilized by Cloud Dataproc within the user-specified region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click Create to create the cluster.\n",
    "\n",
    "Your new cluster will appear in the Clusters list. It may take a few minutes to create, the cluster Status shows as Provisioning until the cluster is ready to use, then changes to Running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a sample Spark job:\n",
    "\n",
    "- Click Jobs in the left pane to switch to Dataproc's jobs view, then click Submit job.\n",
    "\n",
    "- Set the following fields to update Job. Accept the default values for all other fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click Submit.\n",
    "\n",
    "Note: How the job calculates Pi: The Spark job estimates a value of Pi using the Monte Carlo method. It generates x,y points on a coordinate plane that models a circle enclosed by a unit square. The input argument (1000) determines the number of x,y pairs to generate; the more pairs generated, the greater the accuracy of the estimation. This estimation leverages Cloud Dataproc worker nodes to parallelize the computation. For more information, see Estimating Pi using the Monte Carlo Method and see JavaSparkPi.java on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job should appear in the Jobs list, which shows your project's jobs with its cluster, type, and current status. Job status displays as Running, and then Succeeded after it completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the job output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see your completed job's output:\n",
    "\n",
    "- Click the job ID in the Jobs list.\n",
    "\n",
    "- Check Line wrapping or scroll all the way to the right to see the calculated value of Pi. Your output, with Line wrapping checked, should look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the number of worker instances in your cluster:\n",
    "\n",
    "- Select Clusters in the left navigation pane to return to the Dataproc Clusters view.\n",
    "\n",
    "- Click example-cluster in the Clusters list. By default, the page displays an overview of your cluster's CPU usage.\n",
    "\n",
    "- Click Configuration to display your cluster's current settings.\n",
    "\n",
    "- Click Edit. The number of worker nodes is now editable.\n",
    "\n",
    "- Enter 4 in the Worker nodes field.\n",
    "\n",
    "- Click Save.\n",
    "\n",
    "Your cluster is now updated. Check out the number of VM instances in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To rerun the job with the updated cluster, you would click Jobs in the left pane, then click SUBMIT JOB.\n",
    "\n",
    "- Set the same fields you set in the Submit a job section:\n",
    "\n",
    "- Click Submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size = \"6\">Cloud Natural Language API: Qwik Start<center>**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
